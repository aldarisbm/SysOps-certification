## S3 Overview

- Globally Unique
- Regionally provisioned

- Key is fullpath
    - s3://mybucket/[my_file.txt] -> Key
    - s3://mybucket/[hello/world.txt] -> Key

- Key is composed by key prefix + object name
    - prefix /hello
    - object name world.txt

- There's no concept of directories within buckets
    - UI "tricks" you otherwise

- Object Values are the content of the body
    - Max is 5TB
    - Cannot more than 5GB at a time (multi-part upload)

- Metadata (list of text key/value pais - system or user metadata)
- Tags (unicode k/v up to 10)
- Version ID (if versioning is enabled)

- s3 bucket view shows buckets in all regions
- If you right click -> open an s3 object it uses presigned urls

### Versioning
- Has to be enabled at the bucket level
- Same key overwrite will increment the version -> 1 -> 2 -> 3 (intricate string)
- It is best practice to version your buckets
    - Protect against unintended deletes (ability to restore a version)
    - any file that is not versioned prior to enabling versioning will have version `null`
- If you delete an object you get a:
    - delete marker
    - if you delete the `delete marker` it shows again
    - you can delete specific versions

### Encryption
- SSE-S3 - keys handled n managed by AWS
- SSE-KMS - KMS to manage encryption keys
- SSE-C - When you want to manage your own encryption keys
- Client Side Encryption

- SSE-S3
    - Keys handled n managed by AWS s3
    - Object is encrypted server side
    - must set header

- KMS
    - Keys handled n managed by KMS
    - user control + audit trail
    - Object is encrypted server side
    - must set header

- SSE-C
    - S3 does not store the encryption key you provide
    - HTTPS must be used!! (youre sending the key)
    - Encryption key must ve provided in HTTP headers, for every HTTP req made
    - AMazon does not keep the key

- Client Side Encryption
    - Clients library such as the Amazon S3 encrpytion client
    - Clients must encrypt n decrypt the data themselves before sending and after retrieving from s3
    - Client fully manages the key and encryption cycle

- Encryption in flight is also called SSL/TLS

- Default Encrpytion
    - AES-256
    - KMS
    - NONE

### S3 Security
- User based
    - IAM policies (which API calls they are allowe to make)
- Resource based
    - Bucket Policies - bucket wide rules from the s3 console - allows cross acct
- Object ACL
    - finer grain
- Bucket ACL
    - less common

- an IAM principal can access an s3 object if
    - the user IAM policies allow it OR the resource policy allows it
    - AND theres no explicit DENY

- S3 Bucket policy
    - JSON based
    - Resources:
        - Buckets
        - Object
    - Effect
        - Allow
        - Deny
    - Principal
        - Acct or user
    - grant public access to the bucket
    - grant access to another acct
    - force objects to be encrypted on upload

### Bucket settings for Block Public Access
- Block public acces to buckets n objects granted through
    - new acces control lists
    - any access control list
    - new public bucket or access point policies
- Block public and cross-acount access to bucket and objects through any public bucket or access point policies

- S3 Security - Other
    - Supports VPC endpoints (for instances in VPC without interwebs)
    - Loggin and Audit
        - S3 acces logs can be stored in other s3 buck
        - API calls can be logged in AWS cloudtrail
    - User Security
        - MFA DELETE
        - Pre-Signed URLs


### CORS
- Same origin www.example.com/hello && www.example.com/world
- Diff origin api.example.com/hello && www.example.com/world

### S3 Consistency Model
- Eventually consistent
- Read after write consistency for PUTS of new objects
    - PUT 200 -> GET 200
    - GET 404 -> PUT 200 -> GET 404 (eventually consistent) 
- Eventual consistency
    - If you read an object right after updating it, you might get the older version 
        - PUT 200 -> PUT 200 -> GET 200 (might be older version)
        - DELETE 200 -> GET 200! (you might have a successful get - Eventual consistency)
- No way to get strong consistency

### S3 For SysOps

#### Versioning
- S3 vers creates a new version each time you change a file
- that includes when you encrypt a file!
- Its a nice way to get protected against hackers wanting to encrypt our data
- Delete a file in s3 just adds a delete marker
- To delete a bucket you need to delete all of the versions

#### MFA Delete
- NEED FOR:
    - Permanently delete an object version
    - suspend versioning on the bucket
- DONT NEED FOR:
    - enable versioning
    - list deleted versions
- Only the bucket owner (root account) can enable/disable MFA-DELETE
- MFA-Delete currently can only be enabled using the CLI
    - Only enabled with root acct creds
    - Once enabled you can only delete objects (versions) through the CLI with MFA

#### S3 Access logs

- For audit purposes
- Any request made to s3 from any acct auth or denied
- that data can be analyzed using data analytics tools

- Never set your logging bucket to the monitored bucket

#### S3 Replication (CRR & SRR)

- Must enable versioning
- Cross Region Replicatoin
- Same Region Replication
- Buckets in diff accts
- Copying is async
- Must give proper IAM permissions to S3

- CRR uses cases:
    - Compliance
    - Lower latency
    - Repl acc accounts
- SRR use cases
    - Log aggre
    - Live repli between prod and test
- Not retroactive
    - only new objects get replicated
- Delete Operations
    - Not replicated!!
- There's no "chaining" of replication

#### S3 pre-signed URLs

- Can generate pre-signed URLs using SDK or CLI
    - For downloads (CLI)
    - For uploads (SDK)

- Valid for default 3600seconds (1hr) `--expires-in` seconds arg
- Users given a pre-signed URL inherit permissions of the person who generated the URL for GET/PUT

- Examples:
    - Allow only logged-in users to download a premium video on your S3 bucket
    - Allow an ever chaning list of users to download files by generating URLs dynamically
    - Allow temporarily a user to upload a file to a precise location in our bucket

### CloudFront

- Content Delivery Network (CDN)
- Improves read performance (cached at edge)
- 216 points of presence globally (edge locations)

- DDoS protection, integration with Shield, AWS Web App Firewall (wAF
- Can expose external HTTPS and can talk to internal HTTPS backends

#### CloudFront Origins

- S3 Bucket
    - For distrb files and caching them at the edge
    - enhanced security with cloudfront (OAI)
    - CloudFront can be used as an ingress (uploda to s3)
- Custom Origin(HTTP)
    - ALB
    - EC2
    - S3 Website (must enable bucket as static s3 webs)
    - Any HTTP backend you want

#### CloudFront Geo Restriction
- You can restrict who can access
    - Whitelist
    - Blacklist

- The "country" is determined using a 3rd party Geo-IP database
- Use case: Copyrights law to control access to your content

#### CloudFront vs S3 Cros Region Replication
- CloudFront:
    - Global Edge Network
    - Files are cached for a TTL (maybe a day)
    - Great for static content that muts be available everywhere

- S3 CRR:
    - Must be setup for reach region you want repl to happen
    - files are updated in near real time
    - read only
    - great fot dynamic content that needs to be avail at low-latency in a few regions

#### CloudFront Access Logs
- All edge locations access log can go to one log bucket

#### CloudFront Reports
- Possible to generate on
    - caches statistics
    - popular objects
    - top referrers
    - usage
    - viewers
- Based on the access logs data

- Cloufront troubleshooting
    - CloudFront caches HTTP 4xx and 5xx status codes returned by s3

- 4xx error code indicates that user doesnt have access to the underlying bucket (403) or the object is requesting is not found
- 5xx error odes indicates gateway issues

### S3 Inventory
- Amazon s3 inventory helps manage your storage
- Audit and report on the replication and encryption status of your objects
- Use cases
    - business
    - compliance
    - reg needs

- You can query w athena, redshift, presto, hive, spark

- YOu can setup multiple inventories
- data goes from a source bucket to a target bucket (need to setup policy)
- Daily or Weekly! (no way to "trigger" it)

### S3 Storage Classes

- S3 Standard: Gen Purpose
- S3 Standard-Infrequent Access (IA)
- S3 One Zone-Infrequent Access (IA)
- S3 Intelligent tiering
- Amazon Glacier
- Amazon Glacier Deep Archive

- Amazon S3 Reduced Redundancy storage (deprecated)

#### S3 Standard Gen Purpose
- High dura (11 9's)
- If you store 10M objects with amazon s3, you can on average expect to incur a loss of a single object once every 10000 years
- 99.99% Availability over a given year
- Sustain 2 concurrent facility failures
- Big Data Analytics, mobile & gaming apps, content distr

#### S3 Standard IA
- For data that is less freq access, but requires rapid access when needed
- high dura same as gen purpose
- 99.9% availabilty
- Low cost compared to s3 gen purpose

- Data for DR, backups... etc

#### S3 One Zone IA
- Same for IA but data is stored in a single AZ
- High dura same as above (but just one AZ)
- 99.5% Availability
- Low latency n ihght throughput performance
- Supports SSl for data at transit and encr at rest
- Low cost compared to IA (by 20%)

- Storing secondary backup copies of on-premise data or storing data you can recreate

#### S3 Intelligent Tiering
- Same low latency n high throughput
- Small monthly monitoring and auto-tiering fee
- Automatically moves objects between two access tiers based on changing access patterns
- Same Dura
- Resilient against events that impact an entire AZ
- Designed for 99.99% avail over a year

#### Amazon Glacier
- Low cost object storage meant for archiving/backup
- Dta is retained for the logner term (10s of years)
- Alternative to on-promise magnetic tape storage
- Cost per storage per month (0.0004 / GB) + retrieval cost
- Each item in Glacier is called an `Archive` (up to 40tb)
- Archives are stored in Vaults
- Two Tiers
    - Amazon Glacier - 3 retrieval Options
        - Expedited (1 to 5 mins)
        - Standard (3 to 5 hours)
        - Bulk (5 to 12 hours)
        - Minimum storage duration of 90 days
    - Amazon Glacier Deep Archive
        - Standard (12 hours)
        - Bulk (48 hrs)
        - Minimum storage duration of 180 days

#### S3 Lifecycle rules

- Transition actions
    - move to stnd ia after 60 days
    - move to glacier after 6 mnths

- Expiration actions
    - Access log files older than 365
    - delete old versions (if versioning enabled)
    - Delete incomplete multi-part uploads

- Rules can be created for certain prefixes (s3://mybucket/myprefix/*)
- Rules can be created for certain object tags (ex tag Department.finance)

#### S3 Baseline Performance

- automatically scales to high requests, latency 100-200 to get first byte
- Your app can achieve at least 3.5k PUT/COPY/POST/DLETE and 5.5K GET/HEAD requests per second per prefix in a bucket
- There are no limits to the number of prefixes in a bucket
- Example (object path -> prefix)
    - bucket/folder1/sub1/file -> /folder1/sub1/
    - bucket/folder1/sub2/file -> /folder1/sub2/

- If you spread reads across all 2 prefixes evenly, you can achieve 11K per GET n HEAD

- S3 KMS Limitation
    - if you use SSE-KMS u may b impacted by the KMS limits
    - When you upload, it calls the GenerateDataKey KMS API
    - When you download, it calls the Decrypt KMS API

- Count towards the KMS quoate per second (5.5k, 10k, 30k req/s based on region)

#### Performance
- Multi-Part upload:
    - recommended for files > 100MB
        - must use for files > 5GB
    - Can help parallelize uploads (speed up transfers)

- S3 Trasnfer Acceleration (upload only)
    - Increase transfer speed by transferring file to an AWS edge location which will forward the data to the s3 bucket in the target region
    - Compatible with multi-part upload

- S3 Byte-Range Fetches
    - Parallelize GETs by requesting specific byte ranges
    - Better resilience in case of failures

#### S3 Select & Glacier Select
- Retrieve less data using SQL by performing server side filtering
- Can filter by Rows & Columns (simple sql statments)
- less network transfer, less CPU cost client-side

#### S3 Event Notifications
- `S3:ObjectCreated`, `S3:ObjectRemoved`, `S3:ObjectRestore`, `S3:Replication`
- Object name filter possible `*.jpg`
- Use case: generate thumbnails
- SNS, SQS, Lambda
- Can create as many s3 events as desired

- Analytics
    - S3 will attempt to analyze patters on the objects retrievals to give a recommendation on lifecycle policies

#### Glacier
- Low cost object strg meant for archiving/backup
- Data is retained for the longer term (10s of years)
- Alternative to on-prem magnetic tape storage
- 11 9's durability
- Cost per storage per month (0.0004 / GB) + retrieval cost
- Each item in Glacier is called an "Archive"
- Archives are stored in "Vaults"

- Glacier operations
    - Upload
        - Single or multi-part
    - Download
        - First initiate a retrieval job for the particular archive, Glacier prepares the archive for download, user then has a limited time to download
    - Delete
        - Use glacier REST API or SDK by specifying archive ID
    - 3 Retrieval Options
        - Expedited (1 to 5 mins retrieval) - 0.03 per GB or 0.01 per Request
        - Standard (3 - 5 hours) - 0.01 per GB and 0.05 per 1k request
        - Bulk (5 - 12 hours) - 0.0025 per GB and 0.025 per 1k request

- Vault Policies n Vault Lock
    - Vault is a collection of archives
    - Each vault has
        - One vault access policy
        - One vault lock policy
    - vault Policies are written in JSON
    - Vault Access Policy is similar to bucket policy (restrict user/account permissions)
    - Vault Lock Policy is a lock, for reg n compliance reqs
        - Policy is immutable, it can never be changed (thats why its called LOCK)
        - Example
            - Forbit to delete an archive if less than 1 year old
            - Implement WORM (write once read many)

    